{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2373d029",
   "metadata": {},
   "source": [
    "# **Example of using BERT to perform sentiment analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "##Create a sentiment analysis pipeline using a pretrained BERT model.\n",
    "classifier=pipeline(\"sentiment-analysis\"),\n",
    "model=\"bert-base-uncased\"\n",
    "tokenizer=\"bert-base-uncased\"\n",
    "##Test sentences\n",
    "sentences=[\n",
    "    \"I love using BERT for natural language processing tasks!\"\n",
    "    \"I am not a fan of waiting in long lines\"\n",
    "]\n",
    "##Run inference\n",
    "results=classifier(sentences)\n",
    "for sentence, result in zip (sentences, results):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Prediction: {result['label']} | Score: {result['score']:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa8e68",
   "metadata": {},
   "source": [
    "# **OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "261fc7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18b67a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "898ed0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c662a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt='''\n",
    "You are an AI assistant who can perform the following steps:\n",
    "1. Reason through the problem by describing your thoughts in a \"Thought:\" section.\n",
    "2. When you need to use a tool, output an \"Action:\" section with the tool name and its input.\n",
    "3. After the tool call, you'll see an \"Observation:\" section with the tool's output.\n",
    "4. Continue this cycle of Thought → Action → Observation as needed.\n",
    "5. End with a concise \"Final Answer:\" that answers the user's query.\n",
    "\n",
    "Note:\n",
    "- The chain of thought in \"Thought:\" sections is only visible to you and not part of your final answer.\n",
    "- The user should only see your \"Final Answer:\".\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5f6f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = '''\n",
    "What is the weather in Thunder Bay, Ontario, Canada Today?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efeff80e-dfa1-4107-b7a4-81271d67b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client=OpenAI()\n",
    "\n",
    "completion=client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09d955d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought:\n",
      "To find out the weather in Thunder Bay, Ontario, Canada today, I can use a weather-related API to get the current weather information for that location.\n",
      "\n",
      "Action:\n",
      "API Call to get the current weather in Thunder Bay, Ontario, Canada.\n",
      "\n",
      "Observation:\n",
      "The current weather information for Thunder Bay, Ontario, Canada is retrieved.\n",
      "\n",
      "Final Answer:\n",
      "I will provide the current weather in Thunder Bay, Ontario, Canada after using the weather API.\n"
     ]
    }
   ],
   "source": [
    "text = completion.choices[0].message.content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "541c4d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'Action:\\s*(\\w+)\\(\"([^\"]+)\"\\)'\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    tool_name = match.group(1)    # 'GetWeather'\n",
    "    tool_input = match.group(2)   # 'Thunder Bay, Ontario, Canada'\n",
    "    print(\"Tool name:\", tool_name)\n",
    "    print(\"Tool input:\", tool_input)\n",
    "else:\n",
    "    print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "551d0b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually set: tool_name = 'GetWeather', tool_input = 'Thunder Bay, Ontario, Canada'\n"
     ]
    }
   ],
   "source": [
    "tool_name = \"GetWeather\"\n",
    "tool_input = \"Thunder Bay, Ontario, Canada\"\n",
    "print(f\"Manually set: tool_name = '{tool_name}', tool_input = '{tool_input}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c70c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def get_current_weather(city_name):\n",
    "    #base_url = \"https://api.openweathermap.org/data/3.0/onecall\"\n",
    "    #params = {\n",
    "    #    \"lat\": 48.3809,\n",
    "    #    \"lon\": 89.2477,\n",
    "    #    \"appid\": os.environ.get('OPENWEATHERMAPS_API_KEY'),\n",
    "    #    \"units\": \"metric\"  # use \"imperial\" for Fahrenheit\n",
    "    #}\n",
    "\n",
    "    # Make the GET request\n",
    "    #response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # Raise an exception if there's an HTTP error\n",
    "    #response.raise_for_status()\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    #data = response.json()\n",
    "\n",
    "    # Extract relevant fields from the response\n",
    "    #weather_info = {\n",
    "    #    \"city\": data[\"name\"],\n",
    "    #    \"temperature\": data[\"main\"][\"temp\"],\n",
    "    #    \"description\": data[\"weather\"][0][\"description\"],\n",
    "    #    \"humidity\": data[\"main\"][\"humidity\"]\n",
    "    #}\n",
    "    weather_info = {\n",
    "        \"city\": \"Thunder Bay\",\n",
    "        \"temperature\": -5.2,   # in Celsius\n",
    "        \"description\": \"snow\",\n",
    "        \"humidity\": 85         # in percentage\n",
    "    }   \n",
    "    return weather_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72fafd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'city': 'Thunder Bay', 'temperature': -5.2, 'description': 'snow', 'humidity': 85}\n"
     ]
    }
   ],
   "source": [
    "if tool_name == 'GetWeather':\n",
    "    weather_info = get_current_weather(tool_input)\n",
    "    print(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a3b593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought:\n",
      "To find out the weather in Thunder Bay, Ontario, Canada today, I can use a weather-related API to get the current weather information for that location.\n",
      "\n",
      "Action:\n",
      "API Call to get the current weather in Thunder Bay, Ontario, Canada.\n",
      "\n",
      "Observation:\n",
      "The current weather information for Thunder Bay, Ontario, Canada is retrieved.\n",
      "\n",
      "Final Answer:\n",
      "I will provide the current weather in Thunder Bay, Ontario, Canada after using the weather API.\n",
      "\n",
      " Observation: {'city': 'Thunder Bay', 'temperature': -5.2, 'description': 'snow', 'humidity': 85}\n"
     ]
    }
   ],
   "source": [
    "updated_text = text + f\"\\n\\n Observation: {weather_info}\"\n",
    "print(updated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76ee5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",\"content\": user_prompt},\n",
    "        {\"role\": \"assistant\",\"content\": text}, # This is the model's initial simulated response\n",
    "        {\"role\": \"user\",\"content\": updated_text} # This is where the 'Observation' is fed back\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b7de33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "The current weather in Thunder Bay, Ontario, Canada is as follows: \n",
      "- Temperature: -5.2°C\n",
      "- Description: Snow\n",
      "- Humidity: 85%\n"
     ]
    }
   ],
   "source": [
    "text2 = completion.choices[0].message.content\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12847003",
   "metadata": {},
   "source": [
    "# **Anthropic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "333aca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q python-dotenv anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0212534b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "model_id = \"claude-3-5-sonnet-20241022\"\n",
    "\n",
    "messages=[{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"Hello, Claude\",\n",
    "}]\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=model_id,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    messages=messages\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06ed60",
   "metadata": {},
   "source": [
    "# **Cohere**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6286aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q cohere python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38041df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b609b231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='5c3123d6-174d-4e3b-859e-eaf3f25b3d27' finish_reason='COMPLETE' message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text='Hello there! How can I help you today?')], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=3.0, output_tokens=10.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=204.0, output_tokens=10.0)) logprobs=None\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "co = cohere.ClientV2()\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus-08-2024\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello world!\"}],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0427f",
   "metadata": {},
   "source": [
    "# **ai21-Labs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae88974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a278717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from ai21 import AI21Client\n",
    "from ai21.models.chat import ResponseFormat\n",
    "from ai21.models.chat import UserMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e442cd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='chatcmpl-9b014a63-1d6b-57bb-54e4-3ee5796f15cc' choices=[ChatCompletionResponseChoice(index=0, message=AssistantMessage(role='assistant', content=\"The world's smallest mammal is the bumblebee bat, weighing just 2 grams, while the largest is the blue whale, reaching over 150 tons.\", tool_calls=None), logprobs=None, finish_reason='stop')] usage=UsageInfo(prompt_tokens=29, completion_tokens=37, total_tokens=66)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    UserMessage(\n",
    "        content=\"Tell me something I don't know. Limit the response to 30 words maximum.\"\n",
    "    )\n",
    "]\n",
    "client = AI21Client(api_key=os.environ.get(\"AI21_API_KEY\"))\n",
    "response = client.chat.completions.create(\n",
    "\t\tmodel=\"jamba-large\",\n",
    "\t\tmessages=messages,\n",
    "\t\tn=1,\n",
    "\t\tmax_tokens=2048,\n",
    "\t\ttemperature=0.4,\n",
    "\t\ttop_p=1,\n",
    "\t\tresponse_format=ResponseFormat(type=\"text\"),\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49ac30",
   "metadata": {},
   "source": [
    "# **Google AI Studio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "171e4c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d57c9107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911b2469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, or Artificial Intelligence, isn't a single technology but rather a broad field focused on enabling machines to perform tasks that typically require human intelligence.\n",
      "\n",
      "At its core, **AI works by identifying patterns in data and then using those patterns to make predictions, decisions, or generate new content.**\n",
      "\n",
      "Let's break down the fundamental components and processes:\n",
      "\n",
      "---\n",
      "\n",
      "### The Core Idea: Learning from Data\n",
      "\n",
      "Imagine a child learning to identify a cat. They don't start with a rulebook. Instead, they see many examples: fluffy cats, sleek cats, big cats, small cats, cats in different poses. Their brain gradually builds an internal \"model\" of what a cat looks like by observing common features.\n",
      "\n",
      "AI works similarly. Instead of a brain, we use:\n",
      "\n",
      "1.  **Data (The Fuel):** This is the raw information AI learns from. It can be text, images, audio, numbers, videos, etc. The more data, and the higher its quality, the better the AI can learn.\n",
      "    *   **Labeled Data:** Data that has been pre-categorized or tagged (e.g., thousands of photos labeled \"cat\" or \"not cat\"). This is crucial for **supervised learning**.\n",
      "    *   **Unlabeled Data:** Data without explicit tags, where the AI has to find its own patterns (e.g., a collection of news articles the AI needs to group by topic). Used in **unsupervised learning**.\n",
      "\n",
      "2.  **Algorithms (The Recipes):** These are the sets of instructions, rules, or mathematical models that the AI uses to process the data, find patterns, and make decisions. Think of them as the \"learning methods.\"\n",
      "\n",
      "3.  **Compute Power (The Engine):** Training AI models, especially complex ones, requires immense computational resources, often utilizing specialized hardware like GPUs (Graphics Processing Units) that can perform many calculations simultaneously.\n",
      "\n",
      "---\n",
      "\n",
      "### The Learning Process: Training and Inference\n",
      "\n",
      "1.  **Training (Learning Phase):**\n",
      "    *   **Input:** The AI model is fed vast amounts of data.\n",
      "    *   **Pattern Recognition:** The algorithm analyzes this data, looking for correlations, structures, and recurring features.\n",
      "    *   **Weight Adjustment:** Based on the patterns it finds (and often, based on how \"wrong\" its initial guesses are compared to the labeled data), the algorithm adjusts its internal parameters (often called \"weights\" and \"biases\"). This is like fine-tuning knobs on a machine to get the desired output.\n",
      "    *   **Iteration:** This process is repeated thousands or millions of times, with the AI continually refining its internal model to minimize errors and improve accuracy.\n",
      "    *   **Goal:** To create a \"model\" that has learned to map inputs to desired outputs or identify inherent structures.\n",
      "\n",
      "2.  **Inference (Application Phase):**\n",
      "    *   Once trained, the AI model can be deployed to make predictions or perform tasks on new, unseen data.\n",
      "    *   **New Input:** You give the trained model a new piece of data (e.g., a photo it's never seen).\n",
      "    *   **Prediction:** The model applies the patterns it learned during training to this new data and generates an output (e.g., \"This photo contains a cat with 98% confidence\").\n",
      "\n",
      "---\n",
      "\n",
      "### Key Branches and How They Work\n",
      "\n",
      "Most of what people refer to as \"AI\" today falls under the umbrella of **Machine Learning (ML)** and its powerful subfield, **Deep Learning (DL)**.\n",
      "\n",
      "1.  **Machine Learning (ML):**\n",
      "    *   **Supervised Learning:** The most common type. AI learns from labeled examples (input-output pairs).\n",
      "        *   **How it works:** Predicts an output based on known inputs. If you give it historical house data (size, location, bedrooms) with corresponding prices, it can predict the price of a new house.\n",
      "        *   **Examples:** Spam filters, recommending products, predicting stock prices.\n",
      "    *   **Unsupervised Learning:** AI finds patterns and structures in unlabeled data.\n",
      "        *   **How it works:** Groups similar data points together. If you give it customer purchasing data, it might identify different customer segments without being told beforehand what those segments are.\n",
      "        *   **Examples:** Customer segmentation, anomaly detection, data compression.\n",
      "    *   **Reinforcement Learning (RL):** AI learns by trial and error in an environment, receiving rewards for good actions and penalties for bad ones.\n",
      "        *   **How it works:** An \"agent\" performs actions, observes the outcome, and adjusts its strategy to maximize cumulative reward over time.\n",
      "        *   **Examples:** Training robots, self-driving cars (in simulations), game-playing AI (like AlphaGo).\n",
      "\n",
      "2.  **Deep Learning (DL):**\n",
      "    *   A subset of ML that uses **Artificial Neural Networks (ANNs)** with many layers (hence \"deep\").\n",
      "    *   **How it works:** Inspired by the structure of the human brain, ANNs consist of interconnected \"neurons\" organized in layers. Each neuron takes inputs, performs a calculation, and passes the result to neurons in the next layer. The \"deepness\" allows the network to learn complex, hierarchical features from raw data.\n",
      "        *   **Convolutional Neural Networks (CNNs):** Excellent for image recognition. They learn to detect features like edges, shapes, and textures in images.\n",
      "        *   **Recurrent Neural Networks (RNNs) / Transformers:** Designed for sequential data like text or speech. They have a \"memory\" of previous inputs in a sequence, allowing them to understand context. Transformers are particularly powerful for language tasks.\n",
      "    *   **Examples:** Facial recognition, natural language processing (chatbots, translation), speech recognition, generative AI (creating realistic images or text).\n",
      "\n",
      "---\n",
      "\n",
      "### What AI Doesn't Do (Yet)\n",
      "\n",
      "It's crucial to understand that current AI, especially deep learning, is primarily about **statistical pattern recognition**. It excels at finding correlations in data, but it doesn't:\n",
      "\n",
      "*   **Understand meaning** in the human sense. When a chatbot \"understands\" your query, it's really just predicting the most statistically probable response based on its training.\n",
      "*   **Possess common sense** or generalizable intelligence outside its specific training domain.\n",
      "*   **Have consciousness, emotions, or self-awareness.**\n",
      "*   **Infer causation**, only correlation.\n",
      "\n",
      "---\n",
      "\n",
      "### In Summary:\n",
      "\n",
      "AI works by **ingesting massive amounts of data**, using **algorithms** to **identify intricate patterns** within that data during a **training phase**, and then applying those learned patterns to **make predictions, classifications, or generate new content** on new, unseen data during the **inference phase**. It's a powerful tool for automating complex tasks and extracting insights, constantly evolving as more data, better algorithms, and greater compute power become available.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2727632",
   "metadata": {},
   "source": [
    "# **Azure AI Foundry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f35d9313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-inference\n",
      "  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting azure-ai-projects\n",
      "  Downloading azure_ai_projects-1.0.0b12-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting azure-identity\n",
      "  Downloading azure_identity-1.23.1-py3-none-any.whl.metadata (82 kB)\n",
      "Collecting isodate>=0.6.1 (from azure-ai-inference)\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-core>=1.30.0 (from azure-ai-inference)\n",
      "  Downloading azure_core-1.35.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from azure-ai-inference) (4.14.1)\n",
      "Collecting azure-storage-blob>=12.15.0 (from azure-ai-projects)\n",
      "  Downloading azure_storage_blob-12.26.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting azure-ai-agents>=1.0.0 (from azure-ai-projects)\n",
      "  Downloading azure_ai_agents-1.0.2-py3-none-any.whl.metadata (52 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity)\n",
      "  Downloading cryptography-45.0.5-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity)\n",
      "  Downloading msal-1.33.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity)\n",
      "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from azure-core>=1.30.0->azure-ai-inference) (2.32.4)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from azure-core>=1.30.0->azure-ai-inference) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from cryptography>=2.5->azure-identity) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from cffi>=1.14->cryptography>=2.5->azure-identity) (2.22)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2025.7.14)\n",
      "Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
      "Downloading azure_ai_projects-1.0.0b12-py3-none-any.whl (129 kB)\n",
      "Downloading azure_identity-1.23.1-py3-none-any.whl (186 kB)\n",
      "Downloading azure_ai_agents-1.0.2-py3-none-any.whl (189 kB)\n",
      "Downloading azure_core-1.35.0-py3-none-any.whl (210 kB)\n",
      "Downloading azure_storage_blob-12.26.0-py3-none-any.whl (412 kB)\n",
      "Downloading cryptography-45.0.5-cp311-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   ------------ --------------------------- 1.0/3.4 MB 82.7 kB/s eta 0:00:29\n",
      "   ------------ --------------------------- 1.0/3.4 MB 82.7 kB/s eta 0:00:29\n",
      "   ------------ --------------------------- 1.0/3.4 MB 82.7 kB/s eta 0:00:29\n",
      "   --------------- ------------------------ 1.3/3.4 MB 102.9 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 1.3/3.4 MB 102.9 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 1.3/3.4 MB 102.9 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 1.3/3.4 MB 102.9 kB/s eta 0:00:21\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 2.1/3.4 MB 134.9 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 2.1/3.4 MB 134.9 kB/s eta 0:00:10\n",
      "   --------------------------- ------------ 2.4/3.4 MB 148.8 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 2.4/3.4 MB 148.8 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 2.4/3.4 MB 148.8 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 2.6/3.4 MB 159.9 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 2.6/3.4 MB 159.9 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.9/3.4 MB 172.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 2.9/3.4 MB 172.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 2.9/3.4 MB 172.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 2.9/3.4 MB 172.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 3.1/3.4 MB 182.1 kB/s eta 0:00:02\n",
      "   ---------------------------------------- 3.4/3.4 MB 194.7 kB/s eta 0:00:00\n",
      "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Downloading msal-1.33.0-py3-none-any.whl (116 kB)\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: PyJWT, isodate, cryptography, azure-core, azure-storage-blob, azure-ai-inference, azure-ai-agents, msal, azure-ai-projects, msal-extensions, azure-identity\n",
      "\n",
      "   --- ------------------------------------  1/11 [isodate]\n",
      "   ------- --------------------------------  2/11 [cryptography]\n",
      "   ------- --------------------------------  2/11 [cryptography]\n",
      "   ------- --------------------------------  2/11 [cryptography]\n",
      "   ------- --------------------------------  2/11 [cryptography]\n",
      "   ---------- -----------------------------  3/11 [azure-core]\n",
      "   ---------- -----------------------------  3/11 [azure-core]\n",
      "   ---------- -----------------------------  3/11 [azure-core]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   ------------------ ---------------------  5/11 [azure-ai-inference]\n",
      "   ------------------ ---------------------  5/11 [azure-ai-inference]\n",
      "   --------------------- ------------------  6/11 [azure-ai-agents]\n",
      "   --------------------- ------------------  6/11 [azure-ai-agents]\n",
      "   --------------------- ------------------  6/11 [azure-ai-agents]\n",
      "   ------------------------- --------------  7/11 [msal]\n",
      "   ------------------------- --------------  7/11 [msal]\n",
      "   ----------------------------- ----------  8/11 [azure-ai-projects]\n",
      "   ----------------------------- ----------  8/11 [azure-ai-projects]\n",
      "   ----------------------------- ----------  8/11 [azure-ai-projects]\n",
      "   ------------------------------------ --- 10/11 [azure-identity]\n",
      "   ------------------------------------ --- 10/11 [azure-identity]\n",
      "   ------------------------------------ --- 10/11 [azure-identity]\n",
      "   ---------------------------------------- 11/11 [azure-identity]\n",
      "\n",
      "Successfully installed PyJWT-2.10.1 azure-ai-agents-1.0.2 azure-ai-inference-1.0.0b9 azure-ai-projects-1.0.0b12 azure-core-1.35.0 azure-identity-1.23.1 azure-storage-blob-12.26.0 cryptography-45.0.5 isodate-0.7.2 msal-1.33.0 msal-extensions-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-ai-inference azure-ai-projects azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "project_connection_string=\"MY STRING\"\n",
    "\n",
    "project = AIProjectClient.from_connection_string(\n",
    "  conn_str=project_connection_string,\n",
    "  credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6259e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = project.inference.get_chat_completions_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065583c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = chat.complete(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful writing assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write me a poem about flowers\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979f86e",
   "metadata": {},
   "source": [
    "# **Hugging Face Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02f643d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9014cc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Specify the path to env.txt\n",
    "load_dotenv(\"env.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1450da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d585118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d21eb966f5462b907740766d11fe56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1662c9be3f458faea715aeed23ee00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ef4551a848411b9217077c241fb23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccc91a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Bonjour, comment se passe ta journée aujourd'hui ?\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Hello, how is your day going today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c490f1",
   "metadata": {},
   "source": [
    "# **Hugging Face Direct Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd46dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"env.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c69a3f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n403 Client Error. (Request ID: Root=1-68863d81-5a9ed78e2ba1e8ac61315175;e5a540bf-2491-46f2-b174-3f9bc3fbdd64)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.2-1B is awaiting a review from the repo authors.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\utils\\hub.py:470\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    469\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1115\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1114\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1645\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1641\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1642\u001b[39m ):\n\u001b[32m   1643\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1644\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1646\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1647\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1533\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1450\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1449\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1450\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:426\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    423\u001b[39m     message = (\n\u001b[32m    424\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    425\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 403 Client Error. (Request ID: Root=1-68863d81-5a9ed78e2ba1e8ac61315175;e5a540bf-2491-46f2-b174-3f9bc3fbdd64)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.2-1B is awaiting a review from the repo authors.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mD:/Models\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Download the tokenizer and model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Llama-3.2-1B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m model = AutoModelForCausalLM.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[33m\"\u001b[39m, cache_dir=model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1003\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1001\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1197\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1194\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1195\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1197\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1199\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\configuration_utils.py:608\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    607\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    610\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\configuration_utils.py:667\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    663\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    666\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    682\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\utils\\hub.py:312\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    255\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    256\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    257\u001b[39m     **kwargs,\n\u001b[32m    258\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\utils\\hub.py:533\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    532\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    534\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    535\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    536\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n403 Client Error. (Request ID: Root=1-68863d81-5a9ed78e2ba1e8ac61315175;e5a540bf-2491-46f2-b174-3f9bc3fbdd64)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.2-1B is awaiting a review from the repo authors."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"D:/Models\"\n",
    "\n",
    "# Download the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", cache_dir=model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", cache_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23c1165",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m.to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb367b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(prompt, max_new_tokens=50, temperature=0.7, top_p=0.9):\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "  output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p)\n",
    "  output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "  return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the chat loop\n",
    "while True:\n",
    "  user_input = input(\"User: \")\n",
    "  if user_input.lower() == \"quit\":\n",
    "    break\n",
    "  response = generate_text(user_input)\n",
    "  print(\"Llama 3.2:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06e291",
   "metadata": {},
   "source": [
    "# **Hugging Face Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fff7b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5044d668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (2.7.1+cpu)\n",
      "Requirement already satisfied: accelerate in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (4.53.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->transformers[torch]) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers[torch] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d64c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09aca48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb715796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cde9acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tiny dataset subset\n",
    "dataset = load_dataset(\"squad\", split=\"train[:100]\")  # Only 100 examples\n",
    "eval_dataset = load_dataset(\"squad\", split=\"validation[:20]\")  # 20 validation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b55a4cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"  # Much smaller than BERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37115e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize questions and contexts\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=256,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=False\n",
    "    )\n",
    "    \n",
    "    # Initialize answer arrays\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # Process each example in the batch\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        offset = tokenized[\"offset_mapping\"][i]\n",
    "        answer = examples[\"answers\"][i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "\n",
    "        # Find token positions\n",
    "        for idx, (start, end) in enumerate(offset):\n",
    "            if start <= start_char < end:\n",
    "                start_token = idx\n",
    "            if start < end_char <= end:\n",
    "                end_token = idx\n",
    "\n",
    "        # Handle edge cases when answer not found (e.g. due to truncation)\n",
    "        if start_token is None or end_token is None:\n",
    "            start_positions.append(0)  # Usually the [CLS] token index\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_positions.append(start_token)\n",
    "            end_positions.append(end_token)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26318a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126c0af0f31b40d7ba50c8f5d9661214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process datasets\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Explicit batch size\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c0b43e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be754f1694e44dc9a8e33ee262123eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Explicit batch size\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83bf13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./quick-qa-results\",\n",
    "    num_train_epochs=1,  # Single epoch\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,  # Slightly higher learning rate\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"no\",  # Skip evaluation to save time\n",
    "    save_strategy=\"no\",  # Don't save checkpoints\n",
    "    use_cpu=True,  # Force CPU\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard reporting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9210a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9f258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 01:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.616700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./quick-qa-model1\\\\tokenizer_config.json',\n",
       " './quick-qa-model1\\\\special_tokens_map.json',\n",
       " './quick-qa-model1\\\\vocab.txt',\n",
       " './quick-qa-model1\\\\added_tokens.json',\n",
       " './quick-qa-model1\\\\tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and save\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./quick-qa-model\")\n",
    "tokenizer.save_pretrained(\"./quick-qa-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15c5b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_model(model_path=\"./quick-qa-model\"):\n",
    "    # Load model and tokenizer from saved directory\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9dedef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context, model, tokenizer):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=\"only_second\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Find start and end positions\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    # Convert token positions to string\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end + 1], skip_special_tokens=True)\n",
    "  \n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5da557a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_qa_model()\n",
    "\n",
    "# Example context and question\n",
    "context = \"\"\"\n",
    "Python is a high-level programming language created by Guido van Rossum.\n",
    "Python's design emphasizes code readability with its notable use of significant whitespace. \n",
    "Its language constructs and object-oriented approach aim to help programmers write clear, logical code.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who created Python?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35c65cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Who created Python?\n",
      "Answer: guido van rossum\n"
     ]
    }
   ],
   "source": [
    "# Get answer\n",
    "answer = answer_question(question, context, model, tokenizer)\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ce76e",
   "metadata": {},
   "source": [
    "# **Pinecone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c2b846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone[grpc] in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (7.3.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (2025.7.14)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.66.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (1.70.0)\n",
      "Requirement already satisfied: grpcio>=1.59.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (1.74.0)\n",
      "Requirement already satisfied: lz4>=3.1.3 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (4.4.4)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (1.7.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (0.0.7)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.29 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (5.29.5)\n",
      "Requirement already satisfied: protoc-gen-openapiv2<0.0.2,>=0.0.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (0.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from pinecone[grpc]) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (4.14.1)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone[grpc]) (2.5.0)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone[grpc]) (24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone[grpc]) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone[grpc]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone[grpc]) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.5.3->pinecone[grpc]) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"pinecone[grpc]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f3bd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23917fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import time\n",
    "\n",
    "api_key=os.environ.get('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01a809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\": \"vec1\", \"text\": \"Apple is a popular fruit known for its sweetness and crisp texture.\"},\n",
    "    {\"id\": \"vec2\", \"text\": \"The tech company Apple is known for its innovative products like the iPhone.\"},\n",
    "    {\"id\": \"vec3\", \"text\": \"Many people enjoy eating apples as a healthy snack.\"},\n",
    "    {\"id\": \"vec4\", \"text\": \"Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.\"},\n",
    "    {\"id\": \"vec5\", \"text\": \"An apple a day keeps the doctor away, as the saying goes.\"},\n",
    "    {\"id\": \"vec6\", \"text\": \"Apple Computer Company was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne as a partnership.\"}\n",
    "]\n",
    "\n",
    "# Convert the text into numerical vectors that Pinecone can index\n",
    "embeddings = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[d['text'] for d in data],\n",
    "    parameters={\"input_type\": \"passage\", \"truncate\": \"END\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b427cff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingsList(\n",
      "  model='multilingual-e5-large',\n",
      "  vector_type='dense',\n",
      "  data=[\n",
      "    {'vector_type': dense, 'values': [0.04931640625, -0.01328277587890625, ..., -0.0196380615234375, -0.010955810546875]},\n",
      "    {'vector_type': dense, 'values': [0.032562255859375, -0.027862548828125, ..., -0.0200653076171875, -0.021026611328125]},\n",
      "    ... (2 more embeddings) ...,\n",
      "    {'vector_type': dense, 'values': [0.0312347412109375, -0.0185699462890625, ..., -0.02996826171875, -0.032989501953125]},\n",
      "    {'vector_type': dense, 'values': [0.03955078125, -0.01013946533203125, ..., 0.0011348724365234375, -0.04296875]}\n",
      "  ],\n",
      "  usage={'total_tokens': 130}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1460119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a serverless index\n",
    "index_name = \"example-index\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    ) \n",
    "\n",
    "# Wait for the index to be ready\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431d9bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "upserted_count: 6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Target the index where you'll store the vector embeddings\n",
    "index = pc.Index(\"example-index\")\n",
    "\n",
    "# Prepare the records for upsert\n",
    "# Each contains an 'id', the embedding 'values', and the original text as 'metadata'\n",
    "records = []\n",
    "for d, e in zip(data, embeddings):\n",
    "    records.append({\n",
    "        \"id\": d['id'],\n",
    "        \"values\": e['values'],\n",
    "        \"metadata\": {'text': d['text']}\n",
    "    })\n",
    "\n",
    "# Upsert the records into the index\n",
    "index.upsert(\n",
    "    vectors=records,\n",
    "    namespace=\"example-namespace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b34220d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'vec2',\n",
      "              'metadata': {'text': 'The tech company Apple is known for its '\n",
      "                                   'innovative products like the iPhone.'},\n",
      "              'score': 0.87259847,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': []},\n",
      "             {'id': 'vec4',\n",
      "              'metadata': {'text': 'Apple Inc. has revolutionized the tech '\n",
      "                                   'industry with its sleek designs and '\n",
      "                                   'user-friendly interfaces.'},\n",
      "              'score': 0.85148114,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': []},\n",
      "             {'id': 'vec6',\n",
      "              'metadata': {'text': 'Apple Computer Company was founded on '\n",
      "                                   'April 1, 1976, by Steve Jobs, Steve '\n",
      "                                   'Wozniak, and Ronald Wayne as a '\n",
      "                                   'partnership.'},\n",
      "              'score': 0.8501945,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': []}],\n",
      " 'namespace': 'example-namespace',\n",
      " 'usage': {'read_units': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Define your query\n",
    "query = \"Tell me about the tech company known as Apple.\"\n",
    "\n",
    "# Convert the query into a numerical vector that Pinecone can search with\n",
    "query_embedding = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[query],\n",
    "    parameters={\n",
    "        \"input_type\": \"query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Search the index for the three most similar vectors\n",
    "results = index.query(\n",
    "    namespace=\"example-namespace\",\n",
    "    vector=query_embedding[0].values,\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02acfb8",
   "metadata": {},
   "source": [
    "## **Cohere with pinecone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd464800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==3.6.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from datasets==3.6.0) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.32.2->datasets==3.6.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.32.2->datasets==3.6.0) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.66.3->datasets==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58febcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cad8d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1247cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "trec = load_dataset(\"CogComp/trec\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = co.embed(\n",
    "    texts=trec['text'],\n",
    "    model='embed-english-v3.0',\n",
    "    input_type='search_document',\n",
    "    truncate='END'\n",
    ").embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff8f84",
   "metadata": {},
   "source": [
    "# **SerpAPI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ae7c8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from google-search-results) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->google-search-results) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->google-search-results) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->google-search-results) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->google-search-results) (2025.7.14)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py): started\n",
      "  Building wheel for google-search-results (setup.py): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32173 sha256=b53e88590a9da2d7105753c9b82bceed6fdbd44b0a842f529b4d4fc284d2b9f5\n",
      "  Stored in directory: c:\\users\\nutne\\appdata\\local\\pip\\cache\\wheels\\44\\af\\e2\\dde9fab6f1876485b72b35e9cd48da741da67d20e617c3b971\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'google-search-results' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'google-search-results'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "%pip install google-search-results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26fe2639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e2c0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcbe8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  \"engine\": \"google\",\n",
    "  \"q\": \"のは bunpro\",\n",
    "  \"api_key\": os.environ.get('SERP_API_KEY')\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "organic_results = results[\"organic_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52bdd999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"position\": 1,\n",
      "        \"title\": \"Adjective + \\u306e(\\u306f) (JLPT N5)\",\n",
      "        \"link\": \"https://bunpro.jp/grammar_points/adjective-%E3%81%AE-%E3%81%AF\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://bunpro.jp/grammar_points/adjective-%25E3%2581%25AE-%25E3%2581%25AF&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECAwQAQ\",\n",
      "        \"displayed_link\": \"https://bunpro.jp \\u203a grammar_points \\u203a adjective-\\u306e-\\u306f\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd80470395e4e75e212a2ff93b7dcff11381d0313.png\",\n",
      "        \"snippet\": \"One of the roles that the particle \\u306e can take in Japanese is replacing a noun that has already been mentioned, or one that has not been mentioned yet.\",\n",
      "        \"snippet_highlighted_words\": [\n",
      "            \"replacing a noun that has already been mentioned\"\n",
      "        ],\n",
      "        \"source\": \"Bunpro\"\n",
      "    },\n",
      "    {\n",
      "        \"position\": 2,\n",
      "        \"title\": \"\\u306e (JLPT N5)\",\n",
      "        \"link\": \"https://bunpro.jp/grammar_points/%E3%81%AE%E3%81%AF\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://bunpro.jp/grammar_points/%25E3%2581%25AE%25E3%2581%25AF&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECBoQAQ\",\n",
      "        \"displayed_link\": \"https://bunpro.jp \\u203a grammar_points \\u203a \\u306e\\u306f\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd8047039f30396e6f9bf477633f84e3bdda0d851.png\",\n",
      "        \"snippet\": \"This construction can be used after verbs in any tense, except for the polite \\u307e\\u3059 or \\u307e\\u305b\\u3093, which are only used at the end of a sentence, or certain clauses.\",\n",
      "        \"source\": \"Bunpro\"\n",
      "    },\n",
      "    {\n",
      "        \"position\": 3,\n",
      "        \"title\": \"\\u306e(\\u306f) - Grammar Discussion\",\n",
      "        \"link\": \"https://community.bunpro.jp/t/grammar-discussion/503\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://community.bunpro.jp/t/grammar-discussion/503&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECBkQAQ\",\n",
      "        \"displayed_link\": \"https://community.bunpro.jp \\u203a grammar-discussion\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd80470396bab46ad1cf15d96922fc8751a82782a.png\",\n",
      "        \"date\": \"Aug 8, 2018\",\n",
      "        \"snippet\": \"Basically, it is used when you want to use a verb like a noun. It is sometimes called \\u201cnouning\\u201d. For example some constructions (like \\u597d\\u304d) ...\",\n",
      "        \"snippet_highlighted_words\": [\n",
      "            \"used when you want to use a verb like a noun\"\n",
      "        ],\n",
      "        \"source\": \"Bunpro Community\"\n",
      "    },\n",
      "    {\n",
      "        \"position\": 4,\n",
      "        \"title\": \"\\u306e\\u306fX\\u306e\\u65b9\\u3060 (JLPT N3)\",\n",
      "        \"link\": \"https://bunpro.jp/grammar_points/%E3%81%AE%E3%81%AFx%E3%81%AE%E6%96%B9%E3%81%A0\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://bunpro.jp/grammar_points/%25E3%2581%25AE%25E3%2581%25AFx%25E3%2581%25AE%25E6%2596%25B9%25E3%2581%25A0&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECBsQAQ\",\n",
      "        \"displayed_link\": \"https://bunpro.jp \\u203a grammar_points\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd8047039382bcdbabc367708c8c08d7f3efb8e07.png\",\n",
      "        \"snippet\": \"Examples ... \\u5371\\u967a ( \\u304d\\u3051\\u3093 ) \\u306a\\u306e\\u306f\\u30aa\\u30ec\\u306e \\u65b9 ( \\u307b\\u3046 ) \\u3060\\u3088\\u3002 I am the one who is dangerous. \\u4e3b ( \\u304a\\u3082 ) \\u306a \\u8cde ( \\u3057\\u3087\\u3046 ) \\u3092 \\u53d7\\u8cde ( \\u3058\\u3085\\u3057\\u3087\\u3046 ) \\u3059\\u308b\\u306e\\u306f\\u3042\\u3061\\u3083\\u3093\\u306e ...\",\n",
      "        \"snippet_highlighted_words\": [\n",
      "            \"\\u5371\\u967a ( \\u304d\\u3051\\u3093 ) \\u306a\\u306e\\u306f\\u30aa\\u30ec\\u306e \\u65b9 ( \\u307b\\u3046 ) \\u3060\\u3088\"\n",
      "        ],\n",
      "        \"source\": \"Bunpro\"\n",
      "    },\n",
      "    {\n",
      "        \"position\": 5,\n",
      "        \"title\": \"Adjective + \\u306e(\\u306f) - Grammar Discussion\",\n",
      "        \"link\": \"https://community.bunpro.jp/t/adjective-grammar-discussion/6984\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://community.bunpro.jp/t/adjective-grammar-discussion/6984&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECB0QAQ\",\n",
      "        \"displayed_link\": \"https://community.bunpro.jp \\u203a adjective-grammar-discu...\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd8047039be1ed149e329239c1a64c80deda5a6f0.png\",\n",
      "        \"snippet\": \"I don't understand this one lesson, actually I don't understand this whole grammar point. I'm going through Genki and Tae kim along with bunpro for extra ...\",\n",
      "        \"snippet_highlighted_words\": [\n",
      "            \"bunpro\"\n",
      "        ],\n",
      "        \"source\": \"Bunpro Community\"\n",
      "    },\n",
      "    {\n",
      "        \"position\": 6,\n",
      "        \"title\": \"60-Second Bunpro Lesson: \\u306e\\u306f vs. \\u306e\\u304c\",\n",
      "        \"link\": \"https://www.threads.com/@bunprosrs/post/DFD4lo9TkYf?hl=ja\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.threads.com/%40bunprosrs/post/DFD4lo9TkYf%3Fhl%3Dja&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECBwQAQ\",\n",
      "        \"displayed_link\": \"https://www.threads.com \\u203a @bunprosrs\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd80470399cb2691693706606f73c3bc84894b09d.png\",\n",
      "        \"date\": \"Jan 20, 2025\",\n",
      "        \"snippet\": \"What is \\u306e? \\u00b7 In Japanese, \\u306e\\u306f and \\u306e\\u304c work like \\u201cthat\\u201d or \\u201cwhich\\u201d in English. \\u00b7 Examples: \\u00b7 \\u6ca2\\u5c71\\u98df\\u3079\\u308b\\u306e\\u306f \\u2192 \\u201cThe one who eats a lot\\u201d \\u30d0\\u30b9\\u306b ...\",\n",
      "        \"snippet_highlighted_words\": [\n",
      "            \"\\u306e\\u306f and \\u306e\\u304c work like \\u201cthat\\u201d or \\u201cwhich\\u201d in English\"\n",
      "        ],\n",
      "        \"source\": \"Threads\"\n",
      "    },\n",
      "    {\n",
      "        \"position\": 7,\n",
      "        \"title\": \"Bunpro | \\u2728\\u3010(Adjective) + \\u306e\\u306f\\u3011= The one ...\",\n",
      "        \"link\": \"https://www.instagram.com/p/DMYWRQ9IDIt/\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.instagram.com/p/DMYWRQ9IDIt/&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECCUQAQ\",\n",
      "        \"displayed_link\": \"2 likes \\u00b7 1 week ago\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd8047039c5f28bf60ac7ab1d29fb861a6c7a51af.png\",\n",
      "        \"snippet\": \"\\u2728\\u3010(Adjective) + \\u306e\\u306f\\u3011= The one that\\u2026 \\u2728 Want to turn an adjective into \\u201cthe one that\\u2026\\u201d? Just add \\u306e\\u306f after it! Structures:\",\n",
      "        \"snippet_highlighted_words\": [\n",
      "            \"Just add \\u306e\\u306f after it\"\n",
      "        ],\n",
      "        \"source\": \"Instagram \\u00b7 bunprosrs\"\n",
      "    },\n",
      "    {\n",
      "        \"position\": 8,\n",
      "        \"title\": \"Bunpro Japanese SRS - Apps on Google Play\",\n",
      "        \"link\": \"https://play.google.com/store/apps/details?id=bunpro.jp.bunpro_srs\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://play.google.com/store/apps/details%3Fid%3Dbunpro.jp.bunpro_srs&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECDkQAQ\",\n",
      "        \"displayed_link\": \"https://play.google.com \\u203a store \\u203a apps \\u203a details \\u203a id=bun...\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd8047039e8c52aedc72ab55dfadc0687f35d16e7.png\",\n",
      "        \"snippet\": \"This is a great 1 stop app for vocab and grammar. I love the interface, clear and concise grammar explanations, and the automatic input of everything you learn ...\",\n",
      "        \"rich_snippet\": {\n",
      "            \"top\": {\n",
      "                \"detected_extensions\": {\n",
      "                    \"rating\": 4.9,\n",
      "                    \"reviews\": 1703\n",
      "                },\n",
      "                \"extensions\": [\n",
      "                    \"4.9(1,703)\",\n",
      "                    \"Free\",\n",
      "                    \"Android\",\n",
      "                    \"Educational\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"source\": \"Google Play\"\n",
      "    },\n",
      "    {\n",
      "        \"position\": 9,\n",
      "        \"title\": \"\\u306f (JLPT N5)\",\n",
      "        \"link\": \"https://bunpro.jp/grammar_points/%E3%81%AF\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://bunpro.jp/grammar_points/%25E3%2581%25AF&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECDgQAQ\",\n",
      "        \"displayed_link\": \"https://bunpro.jp \\u203a grammar_points \\u203a \\u306f\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd80470390e48d70019d3eb082b17c134c23d961d.png\",\n",
      "        \"snippet\": \"\\u306f has 2 main functions. The first marks the topic of the sentence, the second is used for contrast or making comparisons. Although there are no 'strict' rules ...\",\n",
      "        \"snippet_highlighted_words\": [\n",
      "            \"\\u306f has 2 main functions\"\n",
      "        ],\n",
      "        \"source\": \"Bunpro\"\n",
      "    },\n",
      "    {\n",
      "        \"position\": 10,\n",
      "        \"title\": \"Bunpro N2 (no media)\",\n",
      "        \"link\": \"https://ankiweb.net/shared/info/368455348\",\n",
      "        \"redirect_link\": \"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ankiweb.net/shared/info/368455348&ved=2ahUKEwiQvZ7tlOSOAxXHQzABHZYDC50QFnoECDoQAQ\",\n",
      "        \"displayed_link\": \"https://ankiweb.net \\u203a shared \\u203a info\",\n",
      "        \"favicon\": \"https://serpapi.com/searches/6889d79210b5cf792765cfed/images/baa7d92468b281f5b9e357edd80470397522bfeb81f620dfcca6587a9fa43011.png\",\n",
      "        \"date\": \"Jun 7, 2021\",\n",
      "        \"snippet\": \"At this time, it is not possible to add shared decks directly to your AnkiWeb account - they need to be added from the desktop then synchronized ...\",\n",
      "        \"source\": \"AnkiWeb\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(organic_results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ed320fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from beautifulsoup4) (4.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c89a6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_and_clean(url):\n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error fetching the URL: {e}\"\n",
    "    \n",
    "    # Parse HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get text and clean it\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Clean up the text\n",
    "    # Break into lines and remove leading/trailing space\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # Break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # Drop blank lines\n",
    "    text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1ee5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjective + の(は) (JLPT N5) | BunproLoading user data...BunproGrammar InfoN5 Lesson 10: 7/12(Adjective) + のはThe 'one' that... (Indefinite pronoun, Adjective nominalization)DetailsExamplesResourcesAdjective + の(は)The 'one' that... (Indefinite pronoun, Adjective nominalization)DetailsExamplesResourcesDetailsExamplesResourcesReady to transform your studies?Learn N5 in under a month!Try now, no credit card required!Try BunproLearn MoreStructure［な］Adjective + な + の + は(1)［い］Adjective + の + は(1)(1) が、もDetailsPart of SpeechExpressionWord TypeCase Marking ParticleRegisterStandardAbout Adjective + の(は)One of the roles that the particle の can take in Japanese is replacing a noun that has already been mentioned, or one that has not been mentioned yet. In this way, it is similar to 'the one that (A)' in English. When using this expression, we will need to use な before の, when a な-Adjective is being used. When using this expression, の will be followed by は, が, or も, depending on what the speaker/writer wants to express. Caution の may be used more than once in the same sentence, referring to the same noun. SynonymsExpand AllのはXの方だIt is...that..., It is...who..., ...is the one that..., ...is the one who...N3 GrammarBoth structures single out and highlight something. That is, expressing '(A) is the one that (B)'. Adjective + の(は) is the simpler form, e.g. 新しいのはどっち？ ('Which one is new?') のはXの方だ expands on this to specify and contrast. A common use is in insistent apologies, e.g. いいえ、謝らないといけないのは僕のほうだ ('No, I'm the one who needs to apologize.')のVerb nominalizer, The one who..., That which...N5 GrammarBoth of these patterns use の to turn a phrase into a noun. For example, the verb 走る ('to run') becomes 走るのが好き ('I like running') because the subject marker が must follow a noun. The Adjective + の(は) pattern works similarly, turning adjectives into noun phrases (e.g., 赤い ('red') becomes 赤いのはきれいです ('The red one is pretty')).RelatedExpand AllParticle + のCombined particlesN3 GrammarParticle + の is used when particles like から, と, へ, で, or まで combine with の to describe a noun. For example, 先生への手紙 means 'a letter to (my) teacher,' and 駅までの道 means 'the road to the station.' Adjective + の(は) is a pattern where の replaces a noun, allowing the adjective to form a descriptive phrase. For example, 高いのはこれです means 'This is the expensive one.'Examples--:--SentenceTranslation新しいのは彼の車だ。The one that is new is his car.元気なのは社長です。The one that is energetic is the manager.難しいのは歴史だ。The one that is difficult is history.彼女が素敵なのは服だけだ。The only (one) thing that is great about her is her clothes.暑いのも好きだから、エアコンがなくてもいい。I also like the heat, so it's alright if I don't have an air conditioner.Get more example sentences!Premium users get access to 12 example sentences on all Grammar Points.Try BunproSelf-Study SentencesStudy your own way!Add sentences and study them alongside Bunpro sentences.Try BunproOnlineAdjective + のもJapanese StackExchangeMeaning of 「〜の(は)」 after an adjectiveBunpro Community Discussionの: one (indefinite pronoun)Maggie SenseiOffline[DBJG] A Dictionary of Basic Japanese GrammarPage 315Genki I 2nd EditionPage 233みんなの日本語 IPage 81 [CH 12]Track Resources!Bunpro tracks all of the resources you’ve visited, and offers relevant bookmarks of physical books to help with offline tracking.Try BunproAdjective + の(は) – Grammar DiscussionMost Recent Replies (17 in total)pasiWhy does 「この家の中で一番明るい部屋はどれ？」息子の部屋が一番明るいの not take のは? Is it just because it is at the end of the sentence? The grammar point text doesn’t give any examples of it without the second particle.NinToasterI would guess that its informal speech and thus the は particle is removed to make it easier to say since its already known in the previous sentence (Case Omission).KattosanI’m guessing that is a explanatory の.Got questions about Adjective + の(は)? Join us to discuss, ask, and learn together!Join the DiscussionBunproSimplifying JapaneseOfficial AppsLearn MoreJoin us!CompanyNewsCareersAbout UsTestimonialsAcknowledgementsFor TeachersJapaneseOfficial ForumsContent SearchReading PracticeGrammar ListKana PracticeSupportContact UsSupport & FAQ© 2025, Bunpro SRS. Read our Terms of Service, Privacy Policy and Company Overview.Made with ❤️ from Osaka.English\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "url = organic_results[0]['link']\n",
    "clean_text = scrape_and_clean(url)\n",
    "print(clean_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
