{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2373d029",
   "metadata": {},
   "source": [
    "# **Example of using BERT to perform sentiment analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "##Create a sentiment analysis pipeline using a pretrained BERT model.\n",
    "classifier=pipeline(\"sentiment-analysis\"),\n",
    "model=\"bert-base-uncased\"\n",
    "tokenizer=\"bert-base-uncased\"\n",
    "##Test sentences\n",
    "sentences=[\n",
    "    \"I love using BERT for natural language processing tasks!\"\n",
    "    \"I am not a fan of waiting in long lines\"\n",
    "]\n",
    "##Run inference\n",
    "results=classifier(sentences)\n",
    "for sentence, result in zip (sentences, results):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Prediction: {result['label']} | Score: {result['score']:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa8e68",
   "metadata": {},
   "source": [
    "# **OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "261fc7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18b67a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "898ed0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c662a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt='''\n",
    "You are an AI assistant who can perform the following steps:\n",
    "1. Reason through the problem by describing your thoughts in a \"Thought:\" section.\n",
    "2. When you need to use a tool, output an \"Action:\" section with the tool name and its input.\n",
    "3. After the tool call, you'll see an \"Observation:\" section with the tool's output.\n",
    "4. Continue this cycle of Thought → Action → Observation as needed.\n",
    "5. End with a concise \"Final Answer:\" that answers the user's query.\n",
    "\n",
    "Note:\n",
    "- The chain of thought in \"Thought:\" sections is only visible to you and not part of your final answer.\n",
    "- The user should only see your \"Final Answer:\".\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5f6f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = '''\n",
    "What is the weather in Thunder Bay, Ontario, Canada Today?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efeff80e-dfa1-4107-b7a4-81271d67b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client=OpenAI()\n",
    "\n",
    "completion=client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09d955d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought:\n",
      "To find out the weather in Thunder Bay, Ontario, Canada today, I can use a weather-related API to get the current weather information for that location.\n",
      "\n",
      "Action:\n",
      "API Call to get the current weather in Thunder Bay, Ontario, Canada.\n",
      "\n",
      "Observation:\n",
      "The current weather information for Thunder Bay, Ontario, Canada is retrieved.\n",
      "\n",
      "Final Answer:\n",
      "I will provide the current weather in Thunder Bay, Ontario, Canada after using the weather API.\n"
     ]
    }
   ],
   "source": [
    "text = completion.choices[0].message.content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "541c4d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'Action:\\s*(\\w+)\\(\"([^\"]+)\"\\)'\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    tool_name = match.group(1)    # 'GetWeather'\n",
    "    tool_input = match.group(2)   # 'Thunder Bay, Ontario, Canada'\n",
    "    print(\"Tool name:\", tool_name)\n",
    "    print(\"Tool input:\", tool_input)\n",
    "else:\n",
    "    print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "551d0b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually set: tool_name = 'GetWeather', tool_input = 'Thunder Bay, Ontario, Canada'\n"
     ]
    }
   ],
   "source": [
    "tool_name = \"GetWeather\"\n",
    "tool_input = \"Thunder Bay, Ontario, Canada\"\n",
    "print(f\"Manually set: tool_name = '{tool_name}', tool_input = '{tool_input}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c70c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def get_current_weather(city_name):\n",
    "    #base_url = \"https://api.openweathermap.org/data/3.0/onecall\"\n",
    "    #params = {\n",
    "    #    \"lat\": 48.3809,\n",
    "    #    \"lon\": 89.2477,\n",
    "    #    \"appid\": os.environ.get('OPENWEATHERMAPS_API_KEY'),\n",
    "    #    \"units\": \"metric\"  # use \"imperial\" for Fahrenheit\n",
    "    #}\n",
    "\n",
    "    # Make the GET request\n",
    "    #response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # Raise an exception if there's an HTTP error\n",
    "    #response.raise_for_status()\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    #data = response.json()\n",
    "\n",
    "    # Extract relevant fields from the response\n",
    "    #weather_info = {\n",
    "    #    \"city\": data[\"name\"],\n",
    "    #    \"temperature\": data[\"main\"][\"temp\"],\n",
    "    #    \"description\": data[\"weather\"][0][\"description\"],\n",
    "    #    \"humidity\": data[\"main\"][\"humidity\"]\n",
    "    #}\n",
    "    weather_info = {\n",
    "        \"city\": \"Thunder Bay\",\n",
    "        \"temperature\": -5.2,   # in Celsius\n",
    "        \"description\": \"snow\",\n",
    "        \"humidity\": 85         # in percentage\n",
    "    }   \n",
    "    return weather_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72fafd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'city': 'Thunder Bay', 'temperature': -5.2, 'description': 'snow', 'humidity': 85}\n"
     ]
    }
   ],
   "source": [
    "if tool_name == 'GetWeather':\n",
    "    weather_info = get_current_weather(tool_input)\n",
    "    print(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a3b593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought:\n",
      "To find out the weather in Thunder Bay, Ontario, Canada today, I can use a weather-related API to get the current weather information for that location.\n",
      "\n",
      "Action:\n",
      "API Call to get the current weather in Thunder Bay, Ontario, Canada.\n",
      "\n",
      "Observation:\n",
      "The current weather information for Thunder Bay, Ontario, Canada is retrieved.\n",
      "\n",
      "Final Answer:\n",
      "I will provide the current weather in Thunder Bay, Ontario, Canada after using the weather API.\n",
      "\n",
      " Observation: {'city': 'Thunder Bay', 'temperature': -5.2, 'description': 'snow', 'humidity': 85}\n"
     ]
    }
   ],
   "source": [
    "updated_text = text + f\"\\n\\n Observation: {weather_info}\"\n",
    "print(updated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76ee5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",\"content\": user_prompt},\n",
    "        {\"role\": \"assistant\",\"content\": text}, # This is the model's initial simulated response\n",
    "        {\"role\": \"user\",\"content\": updated_text} # This is where the 'Observation' is fed back\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b7de33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "The current weather in Thunder Bay, Ontario, Canada is as follows: \n",
      "- Temperature: -5.2°C\n",
      "- Description: Snow\n",
      "- Humidity: 85%\n"
     ]
    }
   ],
   "source": [
    "text2 = completion.choices[0].message.content\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12847003",
   "metadata": {},
   "source": [
    "# **Anthropic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "333aca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q python-dotenv anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0212534b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "model_id = \"claude-3-5-sonnet-20241022\"\n",
    "\n",
    "messages=[{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"Hello, Claude\",\n",
    "}]\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=model_id,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    messages=messages\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06ed60",
   "metadata": {},
   "source": [
    "# **Cohere**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6286aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q cohere python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38041df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b609b231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='5c3123d6-174d-4e3b-859e-eaf3f25b3d27' finish_reason='COMPLETE' message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text='Hello there! How can I help you today?')], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=3.0, output_tokens=10.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=204.0, output_tokens=10.0)) logprobs=None\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "co = cohere.ClientV2()\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus-08-2024\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello world!\"}],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0427f",
   "metadata": {},
   "source": [
    "# **ai21-Labs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae88974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a278717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from ai21 import AI21Client\n",
    "from ai21.models.chat import ResponseFormat\n",
    "from ai21.models.chat import UserMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e442cd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='chatcmpl-9b014a63-1d6b-57bb-54e4-3ee5796f15cc' choices=[ChatCompletionResponseChoice(index=0, message=AssistantMessage(role='assistant', content=\"The world's smallest mammal is the bumblebee bat, weighing just 2 grams, while the largest is the blue whale, reaching over 150 tons.\", tool_calls=None), logprobs=None, finish_reason='stop')] usage=UsageInfo(prompt_tokens=29, completion_tokens=37, total_tokens=66)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    UserMessage(\n",
    "        content=\"Tell me something I don't know. Limit the response to 30 words maximum.\"\n",
    "    )\n",
    "]\n",
    "client = AI21Client(api_key=os.environ.get(\"AI21_API_KEY\"))\n",
    "response = client.chat.completions.create(\n",
    "\t\tmodel=\"jamba-large\",\n",
    "\t\tmessages=messages,\n",
    "\t\tn=1,\n",
    "\t\tmax_tokens=2048,\n",
    "\t\ttemperature=0.4,\n",
    "\t\ttop_p=1,\n",
    "\t\tresponse_format=ResponseFormat(type=\"text\"),\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49ac30",
   "metadata": {},
   "source": [
    "# **Google AI Studio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "171e4c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d57c9107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911b2469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, or Artificial Intelligence, isn't a single technology but rather a broad field focused on enabling machines to perform tasks that typically require human intelligence.\n",
      "\n",
      "At its core, **AI works by identifying patterns in data and then using those patterns to make predictions, decisions, or generate new content.**\n",
      "\n",
      "Let's break down the fundamental components and processes:\n",
      "\n",
      "---\n",
      "\n",
      "### The Core Idea: Learning from Data\n",
      "\n",
      "Imagine a child learning to identify a cat. They don't start with a rulebook. Instead, they see many examples: fluffy cats, sleek cats, big cats, small cats, cats in different poses. Their brain gradually builds an internal \"model\" of what a cat looks like by observing common features.\n",
      "\n",
      "AI works similarly. Instead of a brain, we use:\n",
      "\n",
      "1.  **Data (The Fuel):** This is the raw information AI learns from. It can be text, images, audio, numbers, videos, etc. The more data, and the higher its quality, the better the AI can learn.\n",
      "    *   **Labeled Data:** Data that has been pre-categorized or tagged (e.g., thousands of photos labeled \"cat\" or \"not cat\"). This is crucial for **supervised learning**.\n",
      "    *   **Unlabeled Data:** Data without explicit tags, where the AI has to find its own patterns (e.g., a collection of news articles the AI needs to group by topic). Used in **unsupervised learning**.\n",
      "\n",
      "2.  **Algorithms (The Recipes):** These are the sets of instructions, rules, or mathematical models that the AI uses to process the data, find patterns, and make decisions. Think of them as the \"learning methods.\"\n",
      "\n",
      "3.  **Compute Power (The Engine):** Training AI models, especially complex ones, requires immense computational resources, often utilizing specialized hardware like GPUs (Graphics Processing Units) that can perform many calculations simultaneously.\n",
      "\n",
      "---\n",
      "\n",
      "### The Learning Process: Training and Inference\n",
      "\n",
      "1.  **Training (Learning Phase):**\n",
      "    *   **Input:** The AI model is fed vast amounts of data.\n",
      "    *   **Pattern Recognition:** The algorithm analyzes this data, looking for correlations, structures, and recurring features.\n",
      "    *   **Weight Adjustment:** Based on the patterns it finds (and often, based on how \"wrong\" its initial guesses are compared to the labeled data), the algorithm adjusts its internal parameters (often called \"weights\" and \"biases\"). This is like fine-tuning knobs on a machine to get the desired output.\n",
      "    *   **Iteration:** This process is repeated thousands or millions of times, with the AI continually refining its internal model to minimize errors and improve accuracy.\n",
      "    *   **Goal:** To create a \"model\" that has learned to map inputs to desired outputs or identify inherent structures.\n",
      "\n",
      "2.  **Inference (Application Phase):**\n",
      "    *   Once trained, the AI model can be deployed to make predictions or perform tasks on new, unseen data.\n",
      "    *   **New Input:** You give the trained model a new piece of data (e.g., a photo it's never seen).\n",
      "    *   **Prediction:** The model applies the patterns it learned during training to this new data and generates an output (e.g., \"This photo contains a cat with 98% confidence\").\n",
      "\n",
      "---\n",
      "\n",
      "### Key Branches and How They Work\n",
      "\n",
      "Most of what people refer to as \"AI\" today falls under the umbrella of **Machine Learning (ML)** and its powerful subfield, **Deep Learning (DL)**.\n",
      "\n",
      "1.  **Machine Learning (ML):**\n",
      "    *   **Supervised Learning:** The most common type. AI learns from labeled examples (input-output pairs).\n",
      "        *   **How it works:** Predicts an output based on known inputs. If you give it historical house data (size, location, bedrooms) with corresponding prices, it can predict the price of a new house.\n",
      "        *   **Examples:** Spam filters, recommending products, predicting stock prices.\n",
      "    *   **Unsupervised Learning:** AI finds patterns and structures in unlabeled data.\n",
      "        *   **How it works:** Groups similar data points together. If you give it customer purchasing data, it might identify different customer segments without being told beforehand what those segments are.\n",
      "        *   **Examples:** Customer segmentation, anomaly detection, data compression.\n",
      "    *   **Reinforcement Learning (RL):** AI learns by trial and error in an environment, receiving rewards for good actions and penalties for bad ones.\n",
      "        *   **How it works:** An \"agent\" performs actions, observes the outcome, and adjusts its strategy to maximize cumulative reward over time.\n",
      "        *   **Examples:** Training robots, self-driving cars (in simulations), game-playing AI (like AlphaGo).\n",
      "\n",
      "2.  **Deep Learning (DL):**\n",
      "    *   A subset of ML that uses **Artificial Neural Networks (ANNs)** with many layers (hence \"deep\").\n",
      "    *   **How it works:** Inspired by the structure of the human brain, ANNs consist of interconnected \"neurons\" organized in layers. Each neuron takes inputs, performs a calculation, and passes the result to neurons in the next layer. The \"deepness\" allows the network to learn complex, hierarchical features from raw data.\n",
      "        *   **Convolutional Neural Networks (CNNs):** Excellent for image recognition. They learn to detect features like edges, shapes, and textures in images.\n",
      "        *   **Recurrent Neural Networks (RNNs) / Transformers:** Designed for sequential data like text or speech. They have a \"memory\" of previous inputs in a sequence, allowing them to understand context. Transformers are particularly powerful for language tasks.\n",
      "    *   **Examples:** Facial recognition, natural language processing (chatbots, translation), speech recognition, generative AI (creating realistic images or text).\n",
      "\n",
      "---\n",
      "\n",
      "### What AI Doesn't Do (Yet)\n",
      "\n",
      "It's crucial to understand that current AI, especially deep learning, is primarily about **statistical pattern recognition**. It excels at finding correlations in data, but it doesn't:\n",
      "\n",
      "*   **Understand meaning** in the human sense. When a chatbot \"understands\" your query, it's really just predicting the most statistically probable response based on its training.\n",
      "*   **Possess common sense** or generalizable intelligence outside its specific training domain.\n",
      "*   **Have consciousness, emotions, or self-awareness.**\n",
      "*   **Infer causation**, only correlation.\n",
      "\n",
      "---\n",
      "\n",
      "### In Summary:\n",
      "\n",
      "AI works by **ingesting massive amounts of data**, using **algorithms** to **identify intricate patterns** within that data during a **training phase**, and then applying those learned patterns to **make predictions, classifications, or generate new content** on new, unseen data during the **inference phase**. It's a powerful tool for automating complex tasks and extracting insights, constantly evolving as more data, better algorithms, and greater compute power become available.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2727632",
   "metadata": {},
   "source": [
    "# **Azure AI Foundry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f35d9313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-inference\n",
      "  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting azure-ai-projects\n",
      "  Downloading azure_ai_projects-1.0.0b12-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting azure-identity\n",
      "  Downloading azure_identity-1.23.1-py3-none-any.whl.metadata (82 kB)\n",
      "Collecting isodate>=0.6.1 (from azure-ai-inference)\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-core>=1.30.0 (from azure-ai-inference)\n",
      "  Downloading azure_core-1.35.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from azure-ai-inference) (4.14.1)\n",
      "Collecting azure-storage-blob>=12.15.0 (from azure-ai-projects)\n",
      "  Downloading azure_storage_blob-12.26.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting azure-ai-agents>=1.0.0 (from azure-ai-projects)\n",
      "  Downloading azure_ai_agents-1.0.2-py3-none-any.whl.metadata (52 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity)\n",
      "  Downloading cryptography-45.0.5-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity)\n",
      "  Downloading msal-1.33.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity)\n",
      "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from azure-core>=1.30.0->azure-ai-inference) (2.32.4)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from azure-core>=1.30.0->azure-ai-inference) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from cryptography>=2.5->azure-identity) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from cffi>=1.14->cryptography>=2.5->azure-identity) (2.22)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2025.7.14)\n",
      "Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
      "Downloading azure_ai_projects-1.0.0b12-py3-none-any.whl (129 kB)\n",
      "Downloading azure_identity-1.23.1-py3-none-any.whl (186 kB)\n",
      "Downloading azure_ai_agents-1.0.2-py3-none-any.whl (189 kB)\n",
      "Downloading azure_core-1.35.0-py3-none-any.whl (210 kB)\n",
      "Downloading azure_storage_blob-12.26.0-py3-none-any.whl (412 kB)\n",
      "Downloading cryptography-45.0.5-cp311-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 0.5/3.4 MB 225.8 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 0.8/3.4 MB 115.5 kB/s eta 0:00:23\n",
      "   ------------ --------------------------- 1.0/3.4 MB 82.7 kB/s eta 0:00:29\n",
      "   ------------ --------------------------- 1.0/3.4 MB 82.7 kB/s eta 0:00:29\n",
      "   ------------ --------------------------- 1.0/3.4 MB 82.7 kB/s eta 0:00:29\n",
      "   --------------- ------------------------ 1.3/3.4 MB 102.9 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 1.3/3.4 MB 102.9 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 1.3/3.4 MB 102.9 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 1.3/3.4 MB 102.9 kB/s eta 0:00:21\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.6/3.4 MB 118.2 kB/s eta 0:00:16\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.8/3.4 MB 125.0 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 2.1/3.4 MB 134.9 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 2.1/3.4 MB 134.9 kB/s eta 0:00:10\n",
      "   --------------------------- ------------ 2.4/3.4 MB 148.8 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 2.4/3.4 MB 148.8 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 2.4/3.4 MB 148.8 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 2.6/3.4 MB 159.9 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 2.6/3.4 MB 159.9 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.9/3.4 MB 172.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 2.9/3.4 MB 172.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 2.9/3.4 MB 172.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 2.9/3.4 MB 172.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 3.1/3.4 MB 182.1 kB/s eta 0:00:02\n",
      "   ---------------------------------------- 3.4/3.4 MB 194.7 kB/s eta 0:00:00\n",
      "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Downloading msal-1.33.0-py3-none-any.whl (116 kB)\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: PyJWT, isodate, cryptography, azure-core, azure-storage-blob, azure-ai-inference, azure-ai-agents, msal, azure-ai-projects, msal-extensions, azure-identity\n",
      "\n",
      "   --- ------------------------------------  1/11 [isodate]\n",
      "   ------- --------------------------------  2/11 [cryptography]\n",
      "   ------- --------------------------------  2/11 [cryptography]\n",
      "   ------- --------------------------------  2/11 [cryptography]\n",
      "   ------- --------------------------------  2/11 [cryptography]\n",
      "   ---------- -----------------------------  3/11 [azure-core]\n",
      "   ---------- -----------------------------  3/11 [azure-core]\n",
      "   ---------- -----------------------------  3/11 [azure-core]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   -------------- -------------------------  4/11 [azure-storage-blob]\n",
      "   ------------------ ---------------------  5/11 [azure-ai-inference]\n",
      "   ------------------ ---------------------  5/11 [azure-ai-inference]\n",
      "   --------------------- ------------------  6/11 [azure-ai-agents]\n",
      "   --------------------- ------------------  6/11 [azure-ai-agents]\n",
      "   --------------------- ------------------  6/11 [azure-ai-agents]\n",
      "   ------------------------- --------------  7/11 [msal]\n",
      "   ------------------------- --------------  7/11 [msal]\n",
      "   ----------------------------- ----------  8/11 [azure-ai-projects]\n",
      "   ----------------------------- ----------  8/11 [azure-ai-projects]\n",
      "   ----------------------------- ----------  8/11 [azure-ai-projects]\n",
      "   ------------------------------------ --- 10/11 [azure-identity]\n",
      "   ------------------------------------ --- 10/11 [azure-identity]\n",
      "   ------------------------------------ --- 10/11 [azure-identity]\n",
      "   ---------------------------------------- 11/11 [azure-identity]\n",
      "\n",
      "Successfully installed PyJWT-2.10.1 azure-ai-agents-1.0.2 azure-ai-inference-1.0.0b9 azure-ai-projects-1.0.0b12 azure-core-1.35.0 azure-identity-1.23.1 azure-storage-blob-12.26.0 cryptography-45.0.5 isodate-0.7.2 msal-1.33.0 msal-extensions-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-ai-inference azure-ai-projects azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "project_connection_string=\"MY STRING\"\n",
    "\n",
    "project = AIProjectClient.from_connection_string(\n",
    "  conn_str=project_connection_string,\n",
    "  credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6259e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = project.inference.get_chat_completions_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065583c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = chat.complete(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful writing assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write me a poem about flowers\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979f86e",
   "metadata": {},
   "source": [
    "# **Hugging Face Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02f643d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9014cc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Specify the path to env.txt\n",
    "load_dotenv(\"env.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1450da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d585118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d21eb966f5462b907740766d11fe56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1662c9be3f458faea715aeed23ee00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ef4551a848411b9217077c241fb23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccc91a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Bonjour, comment se passe ta journée aujourd'hui ?\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Hello, how is your day going today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c490f1",
   "metadata": {},
   "source": [
    "# **Hugging Face Direct Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd46dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"env.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c69a3f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n403 Client Error. (Request ID: Root=1-68863d81-5a9ed78e2ba1e8ac61315175;e5a540bf-2491-46f2-b174-3f9bc3fbdd64)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.2-1B is awaiting a review from the repo authors.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\utils\\hub.py:470\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    469\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1115\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1114\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1645\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1641\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1642\u001b[39m ):\n\u001b[32m   1643\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1644\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1646\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1647\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1533\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1450\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1449\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1450\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:426\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    423\u001b[39m     message = (\n\u001b[32m    424\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    425\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 403 Client Error. (Request ID: Root=1-68863d81-5a9ed78e2ba1e8ac61315175;e5a540bf-2491-46f2-b174-3f9bc3fbdd64)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.2-1B is awaiting a review from the repo authors.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mD:/Models\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Download the tokenizer and model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Llama-3.2-1B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m model = AutoModelForCausalLM.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[33m\"\u001b[39m, cache_dir=model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1003\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1001\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1197\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1194\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1195\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1197\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1199\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\configuration_utils.py:608\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    607\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    610\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\configuration_utils.py:667\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    663\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    666\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    682\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\utils\\hub.py:312\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    255\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    256\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    257\u001b[39m     **kwargs,\n\u001b[32m    258\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nutne\\anaconda3\\envs\\project1\\Lib\\site-packages\\transformers\\utils\\hub.py:533\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    532\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    534\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    535\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    536\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n403 Client Error. (Request ID: Root=1-68863d81-5a9ed78e2ba1e8ac61315175;e5a540bf-2491-46f2-b174-3f9bc3fbdd64)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.2-1B is awaiting a review from the repo authors."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"D:/Models\"\n",
    "\n",
    "# Download the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", cache_dir=model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", cache_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23c1165",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m.to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb367b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(prompt, max_new_tokens=50, temperature=0.7, top_p=0.9):\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "  output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p)\n",
    "  output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "  return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the chat loop\n",
    "while True:\n",
    "  user_input = input(\"User: \")\n",
    "  if user_input.lower() == \"quit\":\n",
    "    break\n",
    "  response = generate_text(user_input)\n",
    "  print(\"Llama 3.2:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06e291",
   "metadata": {},
   "source": [
    "# **Hugging Face Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fff7b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5044d668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (2.7.1+cpu)\n",
      "Requirement already satisfied: accelerate in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (4.53.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nutne\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nutne\\anaconda3\\envs\\project1\\lib\\site-packages (from requests->transformers[torch]) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers[torch] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d64c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09aca48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb715796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cde9acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tiny dataset subset\n",
    "dataset = load_dataset(\"squad\", split=\"train[:100]\")  # Only 100 examples\n",
    "eval_dataset = load_dataset(\"squad\", split=\"validation[:20]\")  # 20 validation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b55a4cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"  # Much smaller than BERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37115e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize questions and contexts\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=256,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=False\n",
    "    )\n",
    "    \n",
    "    # Initialize answer arrays\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # Process each example in the batch\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        offset = tokenized[\"offset_mapping\"][i]\n",
    "        answer = examples[\"answers\"][i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "\n",
    "        # Find token positions\n",
    "        for idx, (start, end) in enumerate(offset):\n",
    "            if start <= start_char < end:\n",
    "                start_token = idx\n",
    "            if start < end_char <= end:\n",
    "                end_token = idx\n",
    "\n",
    "        # Handle edge cases when answer not found (e.g. due to truncation)\n",
    "        if start_token is None or end_token is None:\n",
    "            start_positions.append(0)  # Usually the [CLS] token index\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_positions.append(start_token)\n",
    "            end_positions.append(end_token)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26318a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126c0af0f31b40d7ba50c8f5d9661214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process datasets\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Explicit batch size\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c0b43e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be754f1694e44dc9a8e33ee262123eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Explicit batch size\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83bf13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./quick-qa-results\",\n",
    "    num_train_epochs=1,  # Single epoch\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,  # Slightly higher learning rate\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"no\",  # Skip evaluation to save time\n",
    "    save_strategy=\"no\",  # Don't save checkpoints\n",
    "    use_cpu=True,  # Force CPU\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard reporting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9210a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9f258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 01:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.616700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./quick-qa-model1\\\\tokenizer_config.json',\n",
       " './quick-qa-model1\\\\special_tokens_map.json',\n",
       " './quick-qa-model1\\\\vocab.txt',\n",
       " './quick-qa-model1\\\\added_tokens.json',\n",
       " './quick-qa-model1\\\\tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and save\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./quick-qa-model\")\n",
    "tokenizer.save_pretrained(\"./quick-qa-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15c5b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_model(model_path=\"./quick-qa-model\"):\n",
    "    # Load model and tokenizer from saved directory\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9dedef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context, model, tokenizer):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=\"only_second\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Find start and end positions\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    # Convert token positions to string\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end + 1], skip_special_tokens=True)\n",
    "  \n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5da557a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_qa_model()\n",
    "\n",
    "# Example context and question\n",
    "context = \"\"\"\n",
    "Python is a high-level programming language created by Guido van Rossum.\n",
    "Python's design emphasizes code readability with its notable use of significant whitespace. \n",
    "Its language constructs and object-oriented approach aim to help programmers write clear, logical code.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who created Python?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35c65cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Who created Python?\n",
      "Answer: guido van rossum\n"
     ]
    }
   ],
   "source": [
    "# Get answer\n",
    "answer = answer_question(question, context, model, tokenizer)\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
